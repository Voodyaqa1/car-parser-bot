import os
import requests
from bs4 import BeautifulSoup
import time
import json
import logging
from telegram import Bot
from telegram.error import TelegramError
import schedule
import threading
from datetime import datetime
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Render
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

class CarParserBot:
    def __init__(self):
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.telegram_token = os.environ.get('TELEGRAM_BOT_TOKEN')  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.chat_id = os.environ.get('TELEGRAM_CHAT_ID')  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è

        if not self.telegram_token or not self.chat_id:
            raise ValueError("TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID must be set")

        self.bot = Bot(token=self.telegram_token)
        self.seen_ads = set()
        self.load_seen_ads()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        self.MIN_PRICE = 300000
        self.MAX_PRICE = 500000
        self.MAX_OWNERS = 2

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def load_seen_ads(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        try:
            with open('/app/seen_ads.json', 'r') as f:
                self.seen_ads = set(json.load(f))
        except (FileNotFoundError, json.JSONDecodeError):
            self.seen_ads = set()

    def save_seen_ads(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        try:
            with open('/app/seen_ads.json', 'w') as f:
                json.dump(list(self.seen_ads), f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è seen_ads: {e}")

    def extract_price(self, price_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä
            price_clean = re.sub(r'[^\d]', '', price_text)
            return int(price_clean) if price_clean else 0
        except:
            return 0

    def extract_owners(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            text_lower = text.lower()
            patterns = [
                r'(\d+)\s*–≤–ª–∞–¥–µ–ª',
                r'(\d+)\s*—Ö–æ–∑—è',
                r'(\d+)\s*—Å–æ–±—Å—Ç–≤–µ–Ω',
                r'–≤–ª–∞–¥–µ–ª[–∞-—è]*\s*:\s*(\d+)'
            ]

            for pattern in patterns:
                match = re.search(pattern, text_lower)
                if match:
                    return int(match.group(1))

            if '–æ–¥–∏–Ω –≤–ª–∞–¥–µ–ª–µ—Ü' in text_lower or '1 –≤–ª–∞–¥–µ–ª–µ—Ü' in text_lower:
                return 1
            elif '–¥–≤–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞' in text_lower or '2 –≤–ª–∞–¥–µ–ª—å—Ü–∞' in text_lower:
                return 2
            elif '—Ç—Ä–∏ –≤–ª–∞–¥–µ–ª—å—Ü–∞' in text_lower or '3 –≤–ª–∞–¥–µ–ª—å—Ü–∞' in text_lower:
                return 3

            return None
        except:
            return None

    def meets_criteria(self, ad):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        price = self.extract_price(ad['price'])
        if price == 0:
            return False, "–¶–µ–Ω–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞"

        if price < self.MIN_PRICE or price > self.MAX_PRICE:
            return False, f"–¶–µ–Ω–∞ {price} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞"

        owners = self.extract_owners(ad.get('full_info', '') + ' ' + ad.get('info', ''))
        if owners is not None and owners > self.MAX_OWNERS:
            return False, f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤: {owners}"

        return True, "OK"

    def parse_drom_general(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ Drom.ru - –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        ads = []
        try:
            urls = [
                "https://www.drom.ru/auto/all/",
                "https://www.drom.ru/auto/all/page2/"
            ]

            for url in urls:
                try:
                    response = requests.get(url, headers=self.headers, timeout=15)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser

                    listings = soup.find_all('a', {'data-ftid': 'bulls-list_bull'})

                    for listing in listings:
                        try:
                            ad_id = listing.get('href', '').split('/')[-1].split('?')[0]
                            if not ad_id or ad_id in self.seen_ads:
                                continue

                            title_elem = listing.find('span', {'data-ftid': 'bull_title'})
                            price_elem = listing.find('span', {'data-ftid': 'bull_price'})
                            info_elem = listing.find('div', {'data-ftid': 'bull_description'})

                            if title_elem and price_elem:
                                title = title_elem.text.strip()
                                price = price_elem.text.strip()
                                info = info_elem.text.strip() if info_elem else ""
                                url_full = "https://www.drom.ru" + listing['href'] if listing['href'].startswith('/') else listing['href']

                                full_info = self.get_drom_details(url_full)

                                ad_data = {
                                    'id': ad_id,
                                    'title': title,
                                    'price': price,
                                    'info': info,
                                    'full_info': full_info,
                                    'url': url_full,
                                    'source': 'Drom.ru'
                                }

                                meets_criteria, reason = self.meets_criteria(ad_data)
                                if meets_criteria:
                                    ads.append(ad_data)

                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Drom: {e}")
                            continue

                    time.sleep(3)

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Drom {url}: {e}")
                    continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Drom: {e}")

        return ads

    def get_drom_details(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å Drom"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser
            info_sections = soup.find_all('div', class_=re.compile(r'info|description|params'))
            return ' '.join([section.get_text() for section in info_sections])
        except:
            return ""

    def parse_auto_ru_general(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ Auto.ru - –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        ads = []
        try:
            urls = [
                "https://auto.ru/moskva/cars/used/",
                "https://auto.ru/moskva/cars/used/?page=2"
            ]

            for url in urls:
                try:
                    response = requests.get(url, headers=self.headers, timeout=15)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser

                    listings = soup.find_all('a', href=re.compile(r'\/auto\.ru\/cars\/used\/sale\/'))

                    for listing in listings:
                        try:
                            href = listing.get('href', '')
                            ad_id = href.split('/')[-2] if '/' in href else href

                            if not ad_id or ad_id in self.seen_ads:
                                continue

                            title_elem = listing.find('span', class_=re.compile(r'Link.*OfferTitle'))
                            price_elem = listing.find('span', class_=re.compile(r'Price'))

                            if title_elem and price_elem:
                                title = title_elem.text.strip()
                                price = price_elem.text.strip()
                                url_full = "https://auto.ru" + href if href.startswith('/') else href

                                info_elems = listing.find_all('span', class_=re.compile(r'ListItem'))
                                info = ' | '.join([elem.text.strip() for elem in info_elems[:3]])

                                full_info = self.get_auto_ru_details(url_full)

                                ad_data = {
                                    'id': ad_id,
                                    'title': title,
                                    'price': price,
                                    'info': info,
                                    'full_info': full_info,
                                    'url': url_full,
                                    'source': 'Auto.ru'
                                }

                                meets_criteria, reason = self.meets_criteria(ad_data)
                                if meets_criteria:
                                    ads.append(ad_data)

                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Auto.ru: {e}")
                            continue

                    time.sleep(3)

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Auto.ru {url}: {e}")
                    continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Auto.ru: {e}")

        return ads

    def get_auto_ru_details(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å Auto.ru"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser
            description_elem = soup.find('div', class_=re.compile(r'Description'))
            if description_elem:
                return description_elem.get_text()
            return ""
        except:
            return ""

    def parse_avito_general(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ Avito.ru - –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        ads = []
        try:
            urls = [
                "https://www.avito.ru/moskva/avtomobili",
                "https://www.avito.ru/moskva/avtomobili?p=2"
            ]

            for url in urls:
                try:
                    response = requests.get(url, headers=self.headers, timeout=15)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser

                    listings = soup.find_all('div', {'data-marker': 'item'})

                    for listing in listings:
                        try:
                            link_elem = listing.find('a', {'data-marker': 'item-title'})
                            if not link_elem:
                                continue

                            href = link_elem.get('href', '')
                            ad_id = href.split('_')[-1] if '_' in href else href.split('/')[-1]

                            if not ad_id or ad_id in self.seen_ads:
                                continue

                            title = link_elem.text.strip()
                            price_elem = listing.find('span', {'data-marker': 'item-price'})
                            price = price_elem.text.strip() if price_elem else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

                            info_elem = listing.find('div', {'class': re.compile(r'.*description.*')})
                            info = info_elem.text.strip() if info_elem else ""

                            url_full = "https://www.avito.ru" + href if href.startswith('/') else href

                            full_info = self.get_avito_details(url_full)

                            ad_data = {
                                'id': ad_id,
                                'title': title,
                                'price': price,
                                'info': info,
                                'full_info': full_info,
                                'url': url_full,
                                'source': 'Avito.ru'
                            }

                            meets_criteria, reason = self.meets_criteria(ad_data)
                            if meets_criteria:
                                ads.append(ad_data)

                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Avito: {e}")
                            continue

                    time.sleep(3)

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Avito {url}: {e}")
                    continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Avito: {e}")

        return ads

    def get_avito_details(self, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å Avito"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')  # –ò–∑–º–µ–Ω–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É–µ–º html.parser
            desc_elem = soup.find('div', {'data-marker': 'item-view/item-description'})
            if desc_elem:
                return desc_elem.get_text()
            return ""
        except:
            return ""

    def format_message(self, ad):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Telegram"""
        price_num = self.extract_price(ad['price'])
        owners = self.extract_owners(ad.get('full_info', '') + ' ' + ad.get('info', ''))

        message = f"üöó *{ad['source']}*\n"
        message += f"üìå *{ad['title']}*\n"
        message += f"üí∞ *{ad['price']}* ({price_num:,} —Ä—É–±.)\n"

        if owners:
            message += f"üë• *–í–ª–∞–¥–µ–ª—å—Ü–µ–≤: {owners}*\n"
        else:
            message += f"üë• *–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–ª–∞–¥–µ–ª—å—Ü–∞—Ö: –Ω–µ —É–∫–∞–∑–∞–Ω–∞*\n"

        if ad['info']:
            message += f"üìù {ad['info'][:100]}...\n" if len(ad['info']) > 100 else f"üìù {ad['info']}\n"

        message += f"üîó [–°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ]({ad['url']})"

        return message

    def send_telegram_message(self, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Telegram"""
        try:
            self.bot.send_message(
                chat_id=self.chat_id,
                text=message,
                parse_mode='Markdown',
                disable_web_page_preview=False
            )
            return True
        except TelegramError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
            return False

    def check_new_ads(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")

        all_ads = []

        try:
            all_ads.extend(self.parse_drom_general())
            all_ads.extend(self.parse_auto_ru_general())
            all_ads.extend(self.parse_avito_general())

            new_ads_count = 0

            for ad in all_ads:
                if ad['id'] not in self.seen_ads:
                    message = self.format_message(ad)
                    if self.send_telegram_message(message):
                        self.seen_ads.add(ad['id'])
                        new_ads_count += 1
                        time.sleep(1)

            if new_ads_count > 0:
                logger.info(f"‚úÖ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {new_ads_count} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
                summary = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {new_ads_count} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π!\n–§–∏–ª—å—Ç—Ä—ã: —Ü–µ–Ω–∞ {self.MIN_PRICE:,}-{self.MAX_PRICE:,} —Ä—É–±., –¥–æ {self.MAX_OWNERS} –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤"
                self.send_telegram_message(summary)
            else:
                logger.info("‚ùå –ù–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

            self.save_seen_ads()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")
            self.send_telegram_message(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {e}")

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –±–æ—Ç–∞"""
        logger.info("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –Ω–∞ Render")
        self.send_telegram_message("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–∞–∑—É –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
        self.check_new_ads()

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        schedule.every(20).minutes.do(self.check_new_ads)

        logger.info("‚è∞ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω (–∫–∞–∂–¥—ã–µ 20 –º–∏–Ω—É—Ç)")

        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                time.sleep(60)

def main():
    try:
        bot = CarParserBot()
        bot.run()
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞: {e}")
        time.sleep(60)
        main()  # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ

if __name__ == "__main__":
    # –î–ª—è Render –Ω—É–∂–Ω–æ —Å–ª—É—à–∞—Ç—å –ø–æ—Ä—Ç
    port = int(os.environ.get("PORT", 5000))

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    bot_thread = threading.Thread(target=main)
    bot_thread.daemon = True
    bot_thread.start()

    # –ü—Ä–æ—Å—Ç–æ–π HTTP —Å–µ—Ä–≤–µ—Ä –¥–ª—è Render
    from flask import Flask

    app = Flask(__name__)

    @app.route('/')
    def home():
        return "Car Parser Bot is running!"

    @app.route('/health')
    def health():
        return "OK"

    app.run(host='0.0.0.0', port=port)